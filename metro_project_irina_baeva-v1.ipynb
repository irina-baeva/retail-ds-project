{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from urllib import request\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.mode.chained_assignment = None #to avoid SettingWithCopyWarning\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns \n",
    "\n",
    "from dateutil.parser import parse\n",
    "from datetime import date\n",
    "import calendar\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import scipy.stats as st\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis of sales and shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "Current report contains work with the dataset provided by \"CODE\" university partner \"Metronom\". \"Metronom\" is a part of the huge german retailer company \"Metro\". The data contains information from the one german store. Time interval of the data set - is a period between 1st to 31th of January 2020. \n",
    "\n",
    "The main question of the report is how sales and shrinkage behave during the current period depending on the weekday, the main category of products, freshness. Results were achieved by exploratory data analysis, visualizations with Python libraries and statistical methods. By exploring the target variables, it was found that sales drops on Fridays, on contrary shrinkage takes the biggest part on Fridays. Fresh products take the main part compared to ultrafresh products. Also, we explored products with the highest shrinkage and sales. Having these results would be important for further study other time intervals and dependencies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "How do we find the main patterns of sales and shrinkage? Is there a relationship between shrinkage and sales? What are the products with the highest shrinkage?  We hypothesised that we could explore relationships between categorical and numerical variables. Following the step by step analyse process, we explored trends of sales and shrinkage by a day and by  a weekday and found that while sales drops on Fridays, shrinkage stays around 42% out of total.  Fresh and ultrafresh products take 56% and 44% of total sales and 73,6% and 26,4% of total shrinkage respectively. The top 3 products with the highest shrinkage take around 7% out of the total shrinkage and make only 0,58% of total sales which could be considered as the reason to give up on selling these products. Exploration of correlation metrics showed that there is no correlation between our target variables. In addition, we found that the most suitable theoretical distribution of the sales is positively skewed lognormal distribution and 75% of the sales values lay in the interval between 1 and 54 euros.\n",
    "\n",
    "In order to achieve the goal we worked with different categories of data and used exploratory data analysis and statistical methods. Since the data was row - cleaning, consulting with the company data experts and finding a way to work with the missing values took a big part of the results. The structure of the report is as follows: loading and preprocessing the data, cleaning the data, exploring the data. Since shrinkage is a very complex variable which depends on human factors, there is not enough information in the current data to tackle the problem of decreasing shrinkage so it could be considered as a target for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data, materials and Methods\n",
    "\n",
    "**Data**\n",
    "\n",
    "The data set was provided by one of the biggest german retail companies __[\"METRO\"](https://www.metro.de/)__ . It contains information about products for the one german department store. Data could not be shared openly due to non-disclosure agreement\n",
    "\n",
    "*Time interval of the data set* -  period between 1st to 31th of January 2020. \n",
    "\n",
    "*German* is a main language of the data set. Names of the columns were translated for more convenient work in the Loading and preprocessing the data. \n",
    "\n",
    "Translation of categories is as following: *ACM FLEISCH* - meat category, *ACM BACKWAREN* - bakery products category, *ACM FRISCHFISCH* - fresh fish category, *ACM MOLKEREIPRODUKTE* - dairy products category, *ACM KAESE* - cheese products category, *ACM FEINKOST* - delicatessen category, *ACM GOURVENIENCE* - the own brand of \"Metro\" category, *ACM WURST* - sausage category, *ACM OBST* - fruits category, *ACM GEMUESE* - vegetables category.\n",
    "\n",
    "Each purchasing area is a part of 2 big categories divided by freshness. We have an information from \"Metro\" with following description:  ACM FLEISCH, ACM FRISCHFISCH, ACM GEMUESE, ACM BLUMEN, ACM OBST belong to \"ultrafresh\" category (5 purchasing areas) and rest of it belong to normal fresh. \n",
    "\n",
    "**Keywords**:\n",
    "\n",
    "*Depo* is a B2B shop for the small businesses (HORECA): shops, cafes, restaurants.\n",
    "\n",
    "*Shrinkage* is a waste of products due to different reasons such as theft, damage, spoilage.\n",
    "\n",
    "*Freshness* is an attribute of the perceived quality of food.\n",
    "\n",
    "\n",
    "**Reqired libraries:**\n",
    "- __[Numpy](https://www.numpy.org/)__ ,\n",
    "- __[Pandas](https://pandas.pydata.org/)__ ,\n",
    "- __[Seaborn](https://seaborn.pydata.org/)__ ,\n",
    "- __[Matplotlib](https://matplotlib.org/)__  ,\n",
    "- __[SciPy](https://www.scipy.org/)__ ,\n",
    "- __[SciPy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html)__ .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Loading and preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./data_metro.csv\"\n",
    "df = pd.read_csv(filepath, encoding = \"ISO-8859-1\", error_bad_lines=False,low_memory=False, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current dataset contains 13 columns, most of them are stored as a string format. A few target columns have missing values.\n",
    "We need to start cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['store_id', 'store_desc', 'CATMAN_BUY_DOMAIN_ID',\n",
       "       'CATMAN_BUY_DOMAIN_DESC', 'PCG_MAIN_CAT_ID', 'PCG_CAT_ID',\n",
       "       'PCG_CAT_DESC', 'PCG_SUB_CAT_ID', 'PCG_SUB_CAT_DESC', 'ART_NO',\n",
       "       'ART_NAME', 'Bv NNEK', 'Bv Stk', 'Bestand Ist NNEK',\n",
       "       'Bestand Ist Stück', 'WA Stück', 'Umsatz VK Netto', 'date_of_day',\n",
       "       'DAY_DESC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have raw data, column names need clarification and should be renamed to more accurately described."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_id</th>\n",
       "      <th>store_desc</th>\n",
       "      <th>purchasing_area_id</th>\n",
       "      <th>purchasing_area_description</th>\n",
       "      <th>main_product_group_id</th>\n",
       "      <th>product_group_category_id</th>\n",
       "      <th>product_group_category_description</th>\n",
       "      <th>product_subgroup_category_id</th>\n",
       "      <th>product_subgroup_category_description</th>\n",
       "      <th>article_number</th>\n",
       "      <th>article_name</th>\n",
       "      <th>daily_shrinkage_in_euros</th>\n",
       "      <th>daily_shrinkage_in_pieces</th>\n",
       "      <th>daily_stock_in_euros</th>\n",
       "      <th>daily_stock_in_pieces</th>\n",
       "      <th>daily_sales_in_pieces</th>\n",
       "      <th>daily_sales_in_euros</th>\n",
       "      <th>date_of_day</th>\n",
       "      <th>DAY_DESC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>362289</td>\n",
       "      <td>579</td>\n",
       "      <td>LIEFERDEPOT BERLIN</td>\n",
       "      <td>63</td>\n",
       "      <td>ACM FLEISCH</td>\n",
       "      <td>971</td>\n",
       "      <td>10</td>\n",
       "      <td>TK-Schweinefleisch</td>\n",
       "      <td>1</td>\n",
       "      <td>Spanferkel</td>\n",
       "      <td>200531</td>\n",
       "      <td>PWHG 968 SANR.INV.VOLL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06.01.20</td>\n",
       "      <td>06.01.20 Mon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>439942</td>\n",
       "      <td>579</td>\n",
       "      <td>LIEFERDEPOT BERLIN</td>\n",
       "      <td>65</td>\n",
       "      <td>ACM OBST</td>\n",
       "      <td>980</td>\n",
       "      <td>7</td>\n",
       "      <td>Obst-Exoten</td>\n",
       "      <td>30</td>\n",
       "      <td>Sonst.Exoten Obst</td>\n",
       "      <td>198978</td>\n",
       "      <td>GRANATAEPFEL 3er</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31.01.20</td>\n",
       "      <td>31.01.20 Fre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        store_id          store_desc  purchasing_area_id  \\\n",
       "362289       579  LIEFERDEPOT BERLIN                  63   \n",
       "439942       579  LIEFERDEPOT BERLIN                  65   \n",
       "\n",
       "       purchasing_area_description  main_product_group_id  \\\n",
       "362289                 ACM FLEISCH                    971   \n",
       "439942                    ACM OBST                    980   \n",
       "\n",
       "        product_group_category_id product_group_category_description  \\\n",
       "362289                         10                 TK-Schweinefleisch   \n",
       "439942                          7                        Obst-Exoten   \n",
       "\n",
       "        product_subgroup_category_id product_subgroup_category_description  \\\n",
       "362289                             1                            Spanferkel   \n",
       "439942                            30                     Sonst.Exoten Obst   \n",
       "\n",
       "        article_number            article_name daily_shrinkage_in_euros  \\\n",
       "362289          200531  PWHG 968 SANR.INV.VOLL                      NaN   \n",
       "439942          198978        GRANATAEPFEL 3er                      NaN   \n",
       "\n",
       "        daily_shrinkage_in_pieces  daily_stock_in_euros  \\\n",
       "362289                        NaN                   0.0   \n",
       "439942                        NaN                   0.0   \n",
       "\n",
       "        daily_stock_in_pieces  daily_sales_in_pieces  daily_sales_in_euros  \\\n",
       "362289                    0.0                    NaN                   NaN   \n",
       "439942                    0.0                    NaN                   NaN   \n",
       "\n",
       "       date_of_day      DAY_DESC  \n",
       "362289    06.01.20  06.01.20 Mon  \n",
       "439942    31.01.20  31.01.20 Fre  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"CATMAN_BUY_DOMAIN_ID\": \"purchasing_area_id\", \n",
    "                   \"CATMAN_BUY_DOMAIN_DESC\" : \"purchasing_area_description\", \n",
    "                   \"PCG_MAIN_CAT_ID\" : \"main_product_group_id\",\n",
    "                   \"PCG_CAT_ID\" : \"product_group_category_id\",\n",
    "                   \"PCG_CAT_DESC\" : \"product_group_category_description\",\n",
    "                   \"PCG_SUB_CAT_ID\" : \"product_subgroup_category_id\",\n",
    "                   \"PCG_SUB_CAT_DESC\" : \"product_subgroup_category_description\",\n",
    "                            \"ART_NO\": \"article_number\",\n",
    "                            \"ART_NAME\":\"article_name\",\n",
    "                   \"Bv NNEK\" : \"daily_shrinkage_in_euros\",\n",
    "                   \"Bv Stk\" : \"daily_shrinkage_in_pieces\",\n",
    "                   \"Bestand Ist NNEK\": \"daily_stock_in_euros\",\n",
    "                   \"Bestand Ist Stück\": \"daily_stock_in_pieces\",\n",
    "                   \"WA Stück\": \"daily_sales_in_pieces\",\n",
    "                   \"Umsatz VK Netto\" : \"daily_sales_in_euros\"\n",
    "                   \n",
    "                  })\n",
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary provided with the dataset is as the following:\n",
    "\n",
    "**store_id and store_desc** - number and name of the department store, we have got a data from one store; \n",
    "\n",
    "**purchasing_area_id** and **purchasing_area_description** - id and name of the main categories of products (11);\n",
    "\n",
    "**product_group_category_id** and **product_group_category_description** - id and name of detailed categories of products (179);\n",
    "\n",
    "**product_subgroup_category_id** and **product_subgroup_category_description** - id and name of  subcategories of products (773);\n",
    "\n",
    "**article_number** and **article_name**- number (23845) and name (23003) of each product;\n",
    "\n",
    "**daily_shrinkage_in_euros** and **daily_shrinkage_in_pieces** - shrinkage of each product in euros and in pieces;\n",
    "\n",
    "**daily_stock_in_euros** and **daily_stock_in_pieces** - daily stock of each product in euros and in pieces;\n",
    "\n",
    "**daily_sales_in_euros** and **daily_sales_in_pieces** - daily sales of each product in euros and in pieces;\n",
    "\n",
    "**date_of_day** - date (YYYY-MM-DD)\n",
    "\n",
    "**DAY_DESC** - date in a different format (DD.MM.YY Weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to study different product categories to familiarize ourselves with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"purchasing_area_description\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 11 purchasing areas of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"product_group_category_description\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 179 product categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"product_subgroup_category_description\"].unique().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 773 product subcategories. \n",
    "\n",
    "Is there the same amount of article numbers as article names?\n",
    "First, there are 738671 rows in total. Now let's check how many unique article numbers and related products in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23845"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_of_art_num = df[\"article_number\"].unique().tolist()\n",
    "len(array_of_art_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check number of names of the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23003"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array_of_art_name = df[\"article_name\"].unique().tolist()\n",
    "len(array_of_art_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got less product names then article numbers. This question was clarified with the company data analyst. It is happening because sometimes the same products has the different article numbers (different taste, flavour and so on). For current report we will use article name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ii. Cleaning the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we explore types of data to find obvious issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store_id                                   int64\n",
       "store_desc                                object\n",
       "purchasing_area_id                         int64\n",
       "purchasing_area_description               object\n",
       "main_product_group_id                      int64\n",
       "product_group_category_id                  int64\n",
       "product_group_category_description        object\n",
       "product_subgroup_category_id               int64\n",
       "product_subgroup_category_description     object\n",
       "article_number                             int64\n",
       "article_name                              object\n",
       "daily_shrinkage_in_euros                  object\n",
       "daily_shrinkage_in_pieces                float64\n",
       "daily_stock_in_euros                     float64\n",
       "daily_stock_in_pieces                    float64\n",
       "daily_sales_in_pieces                    float64\n",
       "daily_sales_in_euros                     float64\n",
       "date_of_day                               object\n",
       "DAY_DESC                                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the types of data frame, we identified several problems that need to be solved in order to continue working with data:\n",
    "* daily_shrinkage_in_euros has an object type;\n",
    "* categorical columns have an object type;\n",
    "* date_of_day has an object type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"daily_shrinkage_in_euros\"].replace(',','.',inplace = True, regex = True)\n",
    "df[[\"daily_shrinkage_in_euros\"]] = df[[\"daily_shrinkage_in_euros\"]].apply(pd.to_numeric)\n",
    "df[\"daily_shrinkage_in_euros\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"purchasing_area_description\", \"product_group_category_description\", \"product_subgroup_category_description\", \"article_name\"]:\n",
    "    df[col] = df[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date is represented by two columns “date_of_day” and “DAY_DESC”. We decided to use the date_of_day column and, in addition to changing the type, extract additional information about the days of the week and save it in a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date_of_day\"]=pd.to_datetime(df[\"date_of_day\"], format=\"%d.%m.%y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when the column \"date_of_day\" has a datetime type, we can extract information about weekdays. This is going to be an important column for the explorational part of the report later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "store_id                                          int64\n",
       "store_desc                                       object\n",
       "purchasing_area_id                                int64\n",
       "purchasing_area_description                    category\n",
       "main_product_group_id                             int64\n",
       "product_group_category_id                         int64\n",
       "product_group_category_description             category\n",
       "product_subgroup_category_id                      int64\n",
       "product_subgroup_category_description          category\n",
       "article_number                                    int64\n",
       "article_name                                   category\n",
       "daily_shrinkage_in_euros                        float64\n",
       "daily_shrinkage_in_pieces                       float64\n",
       "daily_stock_in_euros                            float64\n",
       "daily_stock_in_pieces                           float64\n",
       "daily_sales_in_pieces                           float64\n",
       "daily_sales_in_euros                            float64\n",
       "date_of_day                              datetime64[ns]\n",
       "DAY_DESC                                         object\n",
       "weekday                                        category\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['weekday'] = df[\"date_of_day\"].dt.dayofweek.apply( lambda x: calendar.day_name[x])\n",
    "df['weekday'] = df['weekday'].astype('category')\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of this part, we fixed types of data, but we have some data which is not going to be useful for our report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the null values and redundant data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are columns with a redundant data which is not useful for our project. We can easily drop following colomns: \n",
    "- \"store_id\" and \"store_desc\", since we have data for only one store;\n",
    "- \"DAY_DESC\", since we have enough information about date;\n",
    "- purchasing_area_id, \"main_product_group_id\", \"product_group_category_id\", \"product_subgroup_category_id\", \"article_number\" columns do not have usefull information for the current report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"store_id\",\"store_desc\",\"DAY_DESC\", \"article_number\", \"purchasing_area_id\",\"main_product_group_id\", \"product_group_category_id\",\"product_subgroup_category_id\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchasing_area_description</th>\n",
       "      <th>product_group_category_description</th>\n",
       "      <th>product_subgroup_category_description</th>\n",
       "      <th>article_name</th>\n",
       "      <th>daily_shrinkage_in_euros</th>\n",
       "      <th>daily_shrinkage_in_pieces</th>\n",
       "      <th>daily_stock_in_euros</th>\n",
       "      <th>daily_stock_in_pieces</th>\n",
       "      <th>daily_sales_in_pieces</th>\n",
       "      <th>daily_sales_in_euros</th>\n",
       "      <th>date_of_day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>54182</td>\n",
       "      <td>ACM FRISCHFISCH</td>\n",
       "      <td>Salzwasserfisch</td>\n",
       "      <td>Kabeljau</td>\n",
       "      <td>MSC KABELJAUFILET 200-400g OH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>Saturday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337930</td>\n",
       "      <td>ACM FLEISCH</td>\n",
       "      <td>Rindfleisch Spezial</td>\n",
       "      <td>Rind getreidegefuettert</td>\n",
       "      <td>US BEEF BAVETTE FLAP MEAT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-01-13</td>\n",
       "      <td>Monday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       purchasing_area_description product_group_category_description  \\\n",
       "54182              ACM FRISCHFISCH                    Salzwasserfisch   \n",
       "337930                 ACM FLEISCH                Rindfleisch Spezial   \n",
       "\n",
       "       product_subgroup_category_description                   article_name  \\\n",
       "54182                               Kabeljau  MSC KABELJAUFILET 200-400g OH   \n",
       "337930               Rind getreidegefuettert      US BEEF BAVETTE FLAP MEAT   \n",
       "\n",
       "        daily_shrinkage_in_euros  daily_shrinkage_in_pieces  \\\n",
       "54182                        NaN                        NaN   \n",
       "337930                       NaN                        NaN   \n",
       "\n",
       "        daily_stock_in_euros  daily_stock_in_pieces  daily_sales_in_pieces  \\\n",
       "54182                    0.0                    0.0                    NaN   \n",
       "337930                   0.0                    0.0                    NaN   \n",
       "\n",
       "        daily_sales_in_euros date_of_day   weekday  \n",
       "54182                    NaN  2020-01-04  Saturday  \n",
       "337930                   NaN  2020-01-13    Monday  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when our data looks cleaner, we have to explore our rows on missing values. And decide which method we can use to work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "purchasing_area_description                   0\n",
       "product_group_category_description            0\n",
       "product_subgroup_category_description         0\n",
       "article_name                                  0\n",
       "daily_shrinkage_in_euros                 738074\n",
       "daily_shrinkage_in_pieces                738074\n",
       "daily_stock_in_euros                          0\n",
       "daily_stock_in_pieces                         0\n",
       "daily_sales_in_pieces                    724596\n",
       "daily_sales_in_euros                     724596\n",
       "date_of_day                                   0\n",
       "weekday                                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target columns are \"daily_shrinkage_in_euros\" and \"daily_sales_in_euros\". There are 738074 null values (99%) for \"daily_shrinkage_in_euros\" and 724596 (98%) for daily_sales_in_euros. The main question here is how we can work with null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(597, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"daily_shrinkage_in_euros\"].isnull()== False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14075, 12)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"daily_sales_in_euros\"].isnull()== False].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 597 products that have values for shrinkage and 14075 products that have values for sales. We cannot follow the technique of dropping all null values. \n",
    "- Also we considered a method to estimate some of the missing values by exploring more deeply each product. For this, we checked a few products out of each purchasing area and did not find any pattern which we could use to estimate the missing values. Most of the products do not have a stable day by day record. We did not include massive exploration of each product into the following report, but in general we can use the following function to explore missing values in depth by each article name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gdf = df.groupby([\"article_name\"])\n",
    "def countna(x):\n",
    "    return (x.isna()).sum()\n",
    "gdf.agg(['count', countna, 'size']).sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function above has just approved that for most products there are missing values every day and that's why it is impossible to use any approach for estimation.\n",
    "As a result, to drop data with missing values is not a good choice, because we lose the information and would have ended up with a very small dataset. \n",
    "\n",
    "We made the following decision to check how many columns have both \"NaN\" values for sales and for shrinkage and to drop the rows only in this case. After dropping NaN values, there are 14406 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"daily_shrinkage_in_euros\", \"daily_sales_in_euros\"], how=\"all\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a few consulting sessions with data analysts from \"Metro\". The main insights are as following:\n",
    "- most of the data for shrinkage has been done by employees and has a human factors( wrong inputs);\n",
    "- the reasons behind NaN values for \"daily_shrinkage\" and \"daily_sales\" is missing information which could be filled with zero values and interpreted as \"not exist\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"daily_sales_in_pieces\"] = df[\"daily_sales_in_pieces\"].fillna(0)\n",
    "df[\"daily_sales_in_euros\"] = df[\"daily_sales_in_euros\"].fillna(0)\n",
    "df[\"daily_shrinkage_in_pieces\"] = df[\"daily_shrinkage_in_pieces\"].fillna(0)\n",
    "df[\"daily_shrinkage_in_euros\"] = df[\"daily_shrinkage_in_euros\"].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we discussed business logic behind shrinkage and came to the conclusion, that:\n",
    "- shinkage could be only negative. So positive shrinkage is a mistake of the system or wrong input. \n",
    "- sales could be only positive and negative values is a mistake.\n",
    "\n",
    "Let's determine what percentage of our data has invalid values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"daily_shrinkage_in_euros\"] > 0])/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df[\"daily_sales_in_euros\"] < 0])/df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have explored only 8 rows with positive shrinkage and 93 rows with negative sales. Given that this is less then 1% of data, we will remove these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"daily_shrinkage_in_euros\"] <= 0]\n",
    "df = df[df[\"daily_sales_in_euros\"] >= 0]\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the cleaning process, we have done the following steps:\n",
    "1. changed data types for columns;\n",
    "2. worked with the date column and extracted information about weekdays;\n",
    "3. removed columns with redundant information;\n",
    "4. Worked out rows with the null values by filling missing values for shrinkage in euros and sales in euros with zeroes.\n",
    "5. removed rows with positive values for shrinkage and negative values for sales.\n",
    "\n",
    "We have 14305 rows to analyse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iii. Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring trends of sales, stock and shrinkage\n",
    "\n",
    "We'll begin by exploring our trends of shrinkage and sales by each day during a given month. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_date= df.groupby(\"date_of_day\")[\"daily_shrinkage_in_euros\", \"daily_sales_in_euros\"].sum().reset_index()\n",
    "df_by_date=df_by_date.set_index(\"date_of_day\", drop=True, append=False, inplace=False)\n",
    "df_by_date= df_by_date[df_by_date[\"daily_sales_in_euros\"]>0]\n",
    "df_by_date.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.pointplot( x=df_by_date.index,y='daily_shrinkage_in_euros',data=df_by_date,color='lightblue')\n",
    "sns.pointplot( x=df_by_date.index, y='daily_sales_in_euros',data=df_by_date,color='lightgreen')\n",
    "plt.xlabel('Date, YYYY-MM-DD',fontsize = 12, labelpad=20)\n",
    "plt.ylabel('Total amount by day, EUR',fontsize = 12 )\n",
    "\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(plt.FixedFormatter(df_by_date.index.to_series().dt.strftime(\"%Y-%m-%d\")))\n",
    "shrinkage_patch = mpatches.Patch(color='lightblue', label='Shrinkage')\n",
    "sales_patch = mpatches.Patch(color='lightgreen', label='Sales')\n",
    "plt.legend(handles=[shrinkage_patch, sales_patch])\n",
    " \n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 1. Trends of shrinkage, sales and stock in euros between 1st and 31st of January 2020.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows that shrinkage is staying the same low during a month. There is some raise of shrinkage on the 2nd of January, it could be affected by holidays, probably we could assume that the store had a bigger amount of products for New Year holidays. Also Figure 1 shows sales drop on specific dates (10th, 17 th, 24th of January), it is Friday so we are guessing that there is some dependency on weekdays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring weekdays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the previous step we assumed that sales depends on a weekday. We will explore the assumption more by creating  a new dataframe where the main variables are grouped by a weekday. Department store is not getting any input on Saturday and Sunday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrange = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "df_by_weekday=df.groupby(\"weekday\")[\"daily_shrinkage_in_euros\",\"daily_sales_in_euros\"].sum().reindex(arrange)\n",
    "df_by_weekday.reset_index(inplace=True)\n",
    "df_by_weekday = df_by_weekday.drop(index=[5, 6])\n",
    "df_by_weekday[\"absolute_shrinkage\"] = df_by_weekday[\"daily_shrinkage_in_euros\"].apply(abs)\n",
    "df_by_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_by_weekday.plot.bar(x=\"weekday\", y= [\"absolute_shrinkage\",\"daily_sales_in_euros\"],color=[\"lightblue\", \"lightgreen\"], rot=0, subplots=False, figsize=(5, 7))\n",
    "plt.ylabel('Total amount, EUR',fontsize = 12,color='black')\n",
    "plt.xlabel('Weekday',fontsize = 12, labelpad=20)\n",
    "shrinkage_patch = mpatches.Patch(color = \"lightblue\", label='Shrinkage')\n",
    "sales_patch = mpatches.Patch(color = \"lightgreen\", label='Sales')\n",
    "plt.legend(handles=[shrinkage_patch, sales_patch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 2. Total amount of shrinkage, sales and stock in euros by weekday* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 proves that daily sales on Friday are less than other weekdays. The peak of sales takes place on Mondays and on Thursdays. To make a clear vision of sales and shrinkage, we will find a proportion by weekday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explode = (0.1, 0, 0, 0, 0)\n",
    "plt.pie(df_by_weekday[\"daily_sales_in_euros\"], labels=df_by_weekday['weekday'], autopct='%1.1f%%', startangle=15, shadow = True, explode=my_explode)\n",
    "plt.axis('equal')\n",
    "plt.legend(title=\"Weekday\", loc='upper left', labels=df_by_weekday['weekday'], frameon=True, ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 3. Proportion of sales by weekday* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_explode = (0, 0, 0, 0, 0.1)\n",
    "plt.pie(df_by_weekday[\"absolute_shrinkage\"], labels=df_by_weekday['weekday'], autopct='%1.1f%%', startangle=15, shadow = True, explode=my_explode)\n",
    "plt.axis('equal')\n",
    "plt.legend(title=\"Weekday\", loc='upper left', labels=df_by_weekday['weekday'], frameon=True, ncol=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 4. Proportion of shrinkage by weekday* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, Figure 3 and Figure 4 shows that sales takes the biggest part on Mondays and on Thursdays, the least part on Fridays, on contrary shrinkage is reaching the highest value on Fridays (42,7%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring freshness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['freshness'] = 'fresh'\n",
    "df['freshness'][df['purchasing_area_description'] == 'ACM FLEISCH'] = 'ultrafresh'\n",
    "df['freshness'][df['purchasing_area_description'] == 'ACM FRISCHFISCH'] = 'ultrafresh'\n",
    "df['freshness'][df['purchasing_area_description'] == 'ACM GEMUESE'] = 'ultrafresh'\n",
    "df['freshness'][df['purchasing_area_description'] == 'ACM BLUMEN'] = 'ultrafresh'\n",
    "df['freshness'][df['purchasing_area_description'] == 'ACM OBST'] = 'ultrafresh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have information about each product whether it belongs to the fresh or the ultrafresh category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freshness = df.groupby(\"freshness\")[\"daily_shrinkage_in_euros\", \"daily_sales_in_euros\" ].sum().reset_index()\n",
    "df_freshness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find proportions of fresh and ultra fresh products in the shrinkage (absolute values) and sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pie = pd.DataFrame({'fresh':[16797.2265,394066.109],\n",
    "               'ultrafresh':[6034.9998, 309183.872],\n",
    "               }, index=['daily_shrinkage_in_euros','daily_sales_in_euros' ])    \n",
    "title= ['Daily shrinkage, %','Daily sales, %' ]\n",
    "fig, axs = plt.subplots(nrows=1, ncols=df_pie.index.size, figsize=(15,5))\n",
    "colors = [\"lightblue\",\"lightgreen\"]\n",
    "fig.subplots_adjust(hspace=1.5)\n",
    "\n",
    "\n",
    "for row in range(df_pie.index.size):\n",
    "    fig.add_subplot(axs[row] )\n",
    "    plt.pie(df_pie.loc[df_pie.index[row],:], labels=df_pie.columns, autopct=\"%1.1f%%\", colors=colors)\n",
    "    axs[row].set_title(title[row])\n",
    "fig.legend(title=\"Freshness\", loc='upper right', labels=[\"Fresh\", \"Ultrafresh\"], ncol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 5. Proportion of Fresh and Ultrafresh products in shrinkage, stock and sales* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 5 demonstrates that fresh products take the main part in shrinkage, stock and sales. So following the data for only one month we can conclude that the current depo has less ultrafresh product then normal fresh.  We will also check how many unique products each group has and find out a proportion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby(\"freshness\")[\"article_name\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"freshness\")[\"article_name\"].nunique()/df[\"article_name\"].nunique() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "58,9% of unique products belong to normal fresh and 41,1% belong to the ultrafresh (almost equal parts).\n",
    "\n",
    "On this step, we found out that shrinkage for January 2020 contains a bigger part of the normal fresh products, then the ultrafresh products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the top best-selling products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_by_name_top_sale = df.groupby(\"article_name\")[\"daily_sales_in_euros\",\"daily_stock_in_euros\",\"daily_shrinkage_in_euros\"].sum().reset_index()\n",
    "df_by_name_top_sale[\"% out of total sales\"]= (df_by_name_top_sale[\"daily_sales_in_euros\"]/df_by_name_top_sale[\"daily_sales_in_euros\"].sum())* 100\n",
    "\n",
    "df_by_name_top_sale = df_by_name_top_sale.nlargest(10, [\"daily_sales_in_euros\"])\n",
    "df_by_name_top_sale.set_index(\"article_name\", inplace=True, drop=\"true\")\n",
    "\n",
    "ax = df_by_name_top_sale[\"% out of total sales\"][:10].plot.barh(color=\"lightgreen\")\n",
    "plt.xlabel(\"Proportion of sales for the top 10 products, %\", fontsize = 12, labelpad=20);\n",
    "plt.ylabel(\"Article name\", fontsize = 12,);\n",
    "\n",
    "for patch in ax.patches:\n",
    "    ax.text(\n",
    "        patch.get_width() + 0.1, \n",
    "        patch.get_y() + 0.38,\n",
    "        \" {} %\".format(patch.get_width().round(2)), \n",
    "        fontsize=10,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 6. Top 10 best-selling Products in the department store for January 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sales of products across different categories in the department store, the top 10 best selling products are shown on Figure 6:\n",
    "\n",
    "1. 10er Eier b/w L-M BIO\n",
    "2. MC AVOCADO \n",
    "3. 1L ARO H-SCHLAGSAHNE\n",
    "\n",
    "....\n",
    "\n",
    "Top 10 best-selling products in the department store for January 2020 make almost 11% of total sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_by_name_top_shrinkage = df.groupby(\"article_name\")[\"daily_shrinkage_in_euros\"].sum().reset_index()\n",
    "df_by_name_top_shrinkage[\"% out of total shrinkage\"]= (df_by_name_top_shrinkage[\"daily_shrinkage_in_euros\"]/df_by_name_top_shrinkage[\"daily_shrinkage_in_euros\"].sum())* 100\n",
    "df_by_name_top_shrinkage = df_by_name_top_shrinkage.nsmallest(10, [\"daily_shrinkage_in_euros\"])\n",
    "df_by_name_top_shrinkage.set_index(\"article_name\", inplace=True, drop=\"true\")\n",
    "\n",
    "ax = df_by_name_top_shrinkage[\"% out of total shrinkage\"][:10].plot.barh(color=\"lightblue\")\n",
    "plt.xlabel(\"Proportion of shrinkage for the top 10 products, %\", fontsize = 12, labelpad=20);\n",
    "plt.ylabel(\"Article name\", fontsize = 12);\n",
    "for patch in ax.patches:\n",
    "    ax.text(\n",
    "        patch.get_width() + 0.1, \n",
    "        patch.get_y() + 0.38,\n",
    "        \" {} %\".format(patch.get_width().round(2)), \n",
    "        fontsize=10,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 7. Top 10 products with the highest shrinkage in the department store for January 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the shrinkage of products across different categories in the department store, the top 10 products with highest shrinkage are shown on Figure 7:\n",
    "\n",
    "1. 5 kg Frischer Kloßteig\n",
    "2. 10er Eier weiss/braun L-M\n",
    "3. 500g ARO RAEUCHERLACHS\n",
    "\n",
    "...\n",
    "\n",
    "These 10 products make up around 18% of total shrinkage. \n",
    "Let's explore only the first three products with the highest shrinkage and find out what is the proportion of these products in sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.groupby(\"article_name\")[\"daily_sales_in_euros\"].sum().reset_index()\n",
    "top_3 = [\"5kg FRISCHER KLOSSTEIG BY\",\"10er Eier weiss/braun L-M Bode\", \"500g ARO RAEUCHERLACHS (2)\"]\n",
    "for product in top_3:\n",
    "    print(df_new.loc[df_new[\"article_name\"] == product])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([192, 2026, 1869])/df_new[\"daily_sales_in_euros\"].sum() *100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 best-selling products in the department store for January 2020 make almost 11% of total \n",
    "\n",
    "As a conclusion, the top 3 products with the highest shrinkage take around 7% out of total shrinkage and make 0,58% of total sales. As a suggestion to \"Metro\", to check the reasons for such a high shrinkage of these products and these products could be removed from the store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the main category of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"purchasing_area_description\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMC BLUMEN has no any record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_areas = df.groupby(\"purchasing_area_description\")[\"daily_sales_in_euros\",\"daily_shrinkage_in_euros\"].sum().drop(index=\"ACM BLUMEN\").reset_index()\n",
    "df_areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore purchasing areas in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(data = df_areas, x = \"purchasing_area_description\", y = \"daily_shrinkage_in_euros\")\n",
    "plt.xticks(rotation=-90)\n",
    "plt.xlabel(\"Purchasing areas\",  fontsize = 12, labelpad=20);\n",
    "plt.ylabel(\"Total shrinkage, EUR\", fontsize = 12);\n",
    "plt.title('Shrinkage in EUR by purchasing areas', pad=20,color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 8. Total shrinkage in euros by purchasing area*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Areas with the highest shrinkage (following the Figure 8): ACM WURST, ACM MOLKEREIPRODUKTE and ACM KAESE, areas with the lowest shrinkage is ACM OBST and ACM BACKWAREN. \n",
    "ACM  GOURVENIENCE does not have shrinkage. It is the own brand of \"Metro\", so it could be that the shrinkage counts in a different way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.barplot(data = df_areas, x = \"purchasing_area_description\", y = \"daily_sales_in_euros\")\n",
    "plt.xticks(rotation=-90)\n",
    "plt.xlabel(\"Purchasing areas\",  fontsize = 12, labelpad=20);\n",
    "plt.ylabel(\"Total sales, EUR\",  fontsize = 12);\n",
    "plt.title('Sales in EUR by purchasing areas', pad=20,color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 9. Total sales in euros by purchasing area*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The areas with the highest sales(following Figure 9): ACM MOLKEREIPRODUKTE, ACM KAESE and ACM FLEISCH and with the lowest sales ACM BACKWAREN and ACM GOURVENIENCE which is similar to shrinkage.\n",
    "\n",
    "Let's explore sales by category by plotting boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = df[\"purchasing_area_description\"], y = df[\"daily_sales_in_euros\"])\n",
    "plt.xticks(rotation=-90)\n",
    "plt.xlabel(\"Purchasing areas\",  fontsize = 12, labelpad=20);\n",
    "plt.ylim(0,400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 10. Statistical visualisation of the sales by the main categories of products*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using statistical visualisation of the sales by the main categories of products (Figure 10) we found that we have a lot of outliers. After the research of the topic, it was clarified that it is normal for the real data sets. Here we can see that median is the best average measure for our data set. We can infer from Figure 10 that the highest average of total sales takes the ACM FLEISCH category(63 EUR). We can conclude, that despite the fact that ACM MOLKEREIPRODUKTE has the highest total sales,  ACM Fleish category has the biggest normalised max values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to make a conclusion about correlation between the target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10), dpi= 60)\n",
    "sns.heatmap(df.corr(), xticklabels=df.corr().columns, yticklabels=df.corr().columns, cmap='RdYlGn', center=0, annot=True)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 11. The correllogram with correlation coefficients of all possible pairs of numeric variable*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 11 doesn’t indicate that there’s any strong correlation, because the correlation coefficient of all possible pairs of numeric variables is less than 0.30. Correlation coefficient for sales in euros and shrinkage in euros is +0,025 which is very close to 0. We could prove the absence of correlation by plot (Figure 12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(x=\"daily_sales_in_euros\" , y=\"daily_shrinkage_in_euros\",kind=\"reg\", data=df.sample(1000));\n",
    "plt.ylim([-50,-1]) \n",
    "plt.xlim([1,200])\n",
    "plt.xlabel(\"Total daily sales, EUR\", fontsize = 12, labelpad=20);\n",
    "plt.ylabel(\"Total daily shrinkage, EUR\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 12. Visualisation of relationship between sales and stock and the regression line that best fits the data points.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram on the top of Figure 12 shows the distribution of the total sales. Distribution is positively skewed (mean = 49 is greater than the median = 21).\n",
    "\n",
    "As the last step of the report, let's find what theoretical distribution is the most suitable to describe our sales data? First, let's test sales distributions with a python method and find which distribution fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = df[\"daily_sales_in_euros\"]\n",
    "distributions = [st.alpha,st.beta,st.gamma,st.laplace,st.lognorm,st.pearson3,st.norm]\n",
    "mles = []\n",
    "for distribution in distributions:\n",
    "    pars = distribution.fit(data)\n",
    "    mle = distribution.nnlf(pars, data)\n",
    "    mles.append(mle)\n",
    "results = [(distribution.name, mle) for distribution, mle in zip(distributions, mles)]\n",
    "best_fit = sorted(zip(distributions, mles), key=lambda d: d[1])[0]\n",
    "print(\"Best fit for sales variable reached using {}\".format(best_fit[0].name, best_fit[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithm of the sales variables is normally distributed (Figure 13). We have skewed distribution with low mean values, large variance, and all-positive values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "x = np.log10(df[\"daily_sales_in_euros\"])\n",
    "x = x[np.isfinite(x)]\n",
    "ax = sns.distplot(x, kde=True)\n",
    "plt.xlabel(\"log10('daily_sales_in_euros')\",fontsize = 12, labelpad=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 13. Normal distribution of logarithm of the sales variables ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot lognormal distribution of sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.distplot(df[\"daily_sales_in_euros\"], fit=st.lognorm, kde=False, bins=500)\n",
    "plt.xlabel(\"Total daily sales, EUR\", fontsize = 12, labelpad=20);\n",
    "plt.xlim(0, 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Figure 14. Visualizing sales lognorm distribution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df[\"daily_sales_in_euros\"] > 0]\n",
    "np.quantile(x[\"daily_sales_in_euros\"], 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conclusion, during the last step, we explored shrinkage and sales relation and could not find any correlation. Sales in euros have lognormal distribution, we proved it by plotting normal distribution of logarithm. 75% of the sales are between 1 and 54 euros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "By this exploratory process we answered all the questions that were asked at the beginning of the report, also we made a good ground for further exploration of a longer period of time and tackling problems of shrinkage. Since shrinkage is a very complex and unstable variable, it needs more data from the company and work on site."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
